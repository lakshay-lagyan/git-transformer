# git-transformer
## Handcrafted Transformer Text Autocompleter
A proof of concept Transformer model implemented from first principles in pure Python - no deep-learning frameworks required. This respository walk through every component of original "Attention Is All You Need" paper		

~ Multi-Head Self -Attention

~ Positional Encoding

~ Encoder-Decoder Architecture

~ Feed-Forward Networks

~ Trainig Loop
